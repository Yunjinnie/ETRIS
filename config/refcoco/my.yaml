DATA:
  dataset: refcoco
  train_lmdb: /shared/s2/lab01/dataset/refseg/ETRIS/datasets/lmdb/refcoco/train.lmdb
  train_split: train
  val_lmdb: /shared/s2/lab01/dataset/refseg/ETRIS/datasets/lmdb/refcoco/val.lmdb
  val_split: val
  mask_root: /shared/s2/lab01/dataset/refseg/ETRIS/datasets/masks/refcoco
TRAIN:
  # Base Arch
  text_encoder: bert-base-uncased
  input_size: 480 #416
  word_len: 20 #17
  word_dim: 768
  vis_dim: 512
  num_token: 2
  token_dim: 512
  sync_bn: True
  dropout: 0.
  fusion_drop: 0.
  # Training Setting
  workers: 32  # data loader workers
  workers_val: 16
  epochs: 30
  milestones: [35]
  start_epoch: 0
  batch_size: 8  # batch size for training
  batch_size_val: 32  # batch size for validation during training, memory and speed tradeoff
  base_lr: 0.0001
  lr_decay: 0.1
  lr_multi: 1
  weight_decay: 0.
  warmup_steps: 0.1
  max_norm: 0.
  manual_seed: 0
  print_freq: 100
  # Resume & Save
  exp_name: BERT_DINO
  output_folder: /shared/s2/lab01/dataset/refseg/ETRIS/exp/refcoco
  save_freq: 1
  weight:  # path to initial weight (default: none)
  resume:  # path to latest checkpoint (default: none)
  evaluate: True  # evaluate on validation set, extra gpu memory needed and small batch_size_val is recommend
Distributed:
  dist_url: tcp://localhost:3681
  dist_backend: 'nccl'
  multiprocessing_distributed: True
  world_size: 1
  rank: 0
TEST:
  test_split: val-test
  test_lmdb: /shared/s2/lab01/dataset/refseg/ETRIS/datasets/lmdb/refcoco/val.lmdb
  visualize: False
  attention_map: False

encoder:
  vision_encoder: base_dino
  pretrained_text: true
  pretrained_vision: true
  freeze_text_encoder: true
  freeze_vision_encoder: false
  freeze_proj: false
  unlock_layernorm: false
  add_adapter: false
  adapter_append: false
  fp16: false
  unlock_dense: false
  unlock_attn: false
  unlock_random: false
  bitfit: false
  # Can be set to False if, for example, you are adding adapters
  # to a pretrained model before loading weights (common case).
  load_strict: true
  conventional_adapter:
    insert: false
    reduction_factor: 4
  always_freeze:
    visual_encoder: []
    text_encoder: []
  predictor_depth: 3
  predictor_heads: 8
  use_momentum: true
  moving_average_decay: 0.99